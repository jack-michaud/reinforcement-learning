{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming - Iterative Policy Evaluation\n",
    "\n",
    "Dynamic programming is a method of computing an optimal policy given a *perfect* model of an environment.\n",
    "\n",
    "This is an implementation of Example 4.1 in [Policy Evaluation](http://incompleteideas.net/book/ebook/node41.html). The goal is to get to the corners in the fewest steps.\n",
    "\n",
    "![gridworld](http://incompleteideas.net/book/ebook/imgtmp4.png)\n",
    "\n",
    "I made a value function that uses iterative policy evaluation to update state values.\n",
    "The policy used is a greedy policy based on the value function at that moment in time.\n",
    "\n",
    "\n",
    "In this basic gridworld example, the greedy policy is the optimal policy, so we end up finding the optimal value function $V^*$ through this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld.world import World, Coordinate, actions, Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......\n",
      "......\n",
      "......\n",
      "..X...\n",
      "......\n",
      "......\n"
     ]
    }
   ],
   "source": [
    "x_len = 6\n",
    "y_len = 6\n",
    "def reward_function(coordinate: Coordinate):\n",
    "    return -1\n",
    "\n",
    "def initialize_world(initial_position=Coordinate(2, 3)):\n",
    "    world = World(x_len, y_len, reward_function, initial_position=initial_position)\n",
    "    world.print()\n",
    "    return world\n",
    "\n",
    "world = initialize_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the iterative policy evaluation loop.\n",
    "\n",
    "I use a form of the Bellman equation for the greedy policy $\\pi$. \n",
    "\n",
    "$$ V_{k+1} = \\sum_a \\pi(s, a) \\sum_{s'} P^a_{ss'} [ R^a_{ss'} + \\gamma V_k (s')] $$\n",
    "\n",
    "- $\\pi(s, a)$ is the probabillity of taking an action $a$ from that state $s$. The greedy policy will return a list of actions that are greedy given the value function $V_k$. Since the probability of selecting either of these actions is equal, this manifests in the algorithm by dividing the sum by the count of the possible actions.\n",
    "- $P^a_{ss'}$ is the probability that taking action $a$ will result in state $s$ transitioning to state $s'$. Our gridworld is deterministic, so this is 1 and is ignored in the algorithm.\n",
    "- $R_{ss'}^a$ is the reward of the state transition from state $s$ to state $s'$. In this environment, it's always -1.\n",
    "- $V_k(s')$ is the old value function's determination of the next state value. It's multiplied by $\\gamma$ (some number less than 1 and greater than 0) to assure convergence.\n",
    "\n",
    "I iterate until $\\Delta$ is smaller than some small number; it ends up taking only 6 iterations to converge entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "state_value_map = [[0 for x in range(x_len)] for y in range(y_len)]\n",
    "def value_function(state: Coordinate) -> float:\n",
    "    return state_value_map[state.x][state.y]\n",
    "\n",
    "def policy_function(state: Coordinate) -> List[Direction]:\n",
    "    # Returns a list of actions that maximize the value function. \n",
    "    # If there are multiple actions that maximize the value function, return them all.\n",
    "    max_value = -float('inf')\n",
    "    max_actions = []\n",
    "    for action in actions:\n",
    "        next_coordinate = world.move_direction(state, action)\n",
    "        value = value_function(next_coordinate)\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            max_actions = [action]\n",
    "        elif value == max_value:\n",
    "            max_actions.append(action)\n",
    "    return max_actions\n",
    "\n",
    "gamma = 0.9\n",
    "def update_value_function():\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        value_function_delta = 0\n",
    "        for x in range(x_len):\n",
    "            for y in range(y_len):\n",
    "                state = Coordinate(x, y)\n",
    "                # Assuming a Greedy policy with respect to the current value function (`policy_function`)\n",
    "                # Incrementally updating the state value map for faster convergence\n",
    "                old_state = state_value_map[x][y]\n",
    "                # Get possible actions for the current state\n",
    "                # If the current state is in the goal state, then the value function is 0\n",
    "                if state == Coordinate(x_len - 1, y_len - 1):\n",
    "                    state_value_map[x][y] = 0\n",
    "                elif state == Coordinate(0, 0):\n",
    "                    state_value_map[x][y] = 0\n",
    "                else:\n",
    "                    possible_actions = policy_function(state)\n",
    "                    action_count = len(possible_actions)\n",
    "                    state_value_map[x][y] = float(np.array([-1 + (gamma * value_function(world.move_direction(state, action))) for action in possible_actions]).sum()) / action_count\n",
    "                value_function_delta = max(value_function_delta, abs(old_state - state_value_map[x][y]))\n",
    "\n",
    "        if value_function_delta < 0.00001:\n",
    "            break\n",
    "\n",
    "    return iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value function:\n",
      "0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
      "After 6 iteration of updating the value function:\n",
      "0.0, -1.0, -1.9, -2.7, -3.4, -4.1, \n",
      "-1.0, -1.9, -2.7, -3.4, -4.1, -3.4, \n",
      "-1.9, -2.7, -3.4, -4.1, -3.4, -2.7, \n",
      "-2.7, -3.4, -4.1, -3.4, -2.7, -1.9, \n",
      "-3.4, -4.1, -3.4, -2.7, -1.9, -1.0, \n",
      "-4.1, -3.4, -2.7, -1.9, -1.0, 0.0, \n"
     ]
    }
   ],
   "source": [
    "def print_state_value_map():\n",
    "    for row in state_value_map:\n",
    "        for cell in row:\n",
    "            print(f\"{cell:.1f}\", end=\", \")\n",
    "        print()\n",
    "\n",
    "\n",
    "print(\"Initial value function:\")\n",
    "print_state_value_map()\n",
    "\n",
    "iterations = update_value_function()\n",
    "\n",
    "print(f\"After {iterations} iteration of updating the value function:\")\n",
    "print_state_value_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......\n",
      "......\n",
      "......\n",
      "..X...\n",
      "......\n",
      "......\n",
      "--------\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      ".....X\n",
      "Made it in 5 steps!\n"
     ]
    }
   ],
   "source": [
    "world = initialize_world()\n",
    "\n",
    "def step_with_policy_function():\n",
    "    directions = policy_function(world.current_position)\n",
    "    direction = np.random.choice(directions)\n",
    "    world.move(direction)\n",
    "\n",
    "steps = 0\n",
    "while not (world.current_position == Coordinate(x_len - 1, y_len - 1) or world.current_position == Coordinate(0, 0)):\n",
    "    steps += 1\n",
    "    step_with_policy_function()\n",
    "\n",
    "print(\"-\" * 8)\n",
    "world.print()\n",
    "print(f\"Made it in {steps} steps!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fedc1f2bd9444f11e1e6daecfe0cb6f38e908e4b5606429ea0d1cece8a1f0f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
